{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHmIQe4iFtUu"
   },
   "source": [
    "\n",
    "\n",
    "# Preprocessing & Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJca2wocFtU4"
   },
   "source": [
    "### Data Importation & fields selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "628bEecRGf4p",
    "outputId": "33eee126-e52f-4a0d-f2ea-f6a21c46a5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Tashaphyne\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/80/8b94a7d91c92d252cc395d80ebf9e4b2fa5bdd0c3fbf10b4a1749817c998/Tashaphyne-0.3.4.1-py3-none-any.whl (244kB)\n",
      "\r",
      "\u001b[K     |â–ˆâ–                              | 10kB 16.7MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–Š                             | 20kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆ                            | 30kB 12.4MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 40kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 51kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 61kB 6.1MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 71kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 81kB 6.3MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 92kB 6.8MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 102kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 112kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 122kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 133kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 143kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 153kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 163kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 174kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 184kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 194kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 204kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 225kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 235kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 6.6MB/s \n",
      "\u001b[?25hCollecting pyarabic\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 19.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
      "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp36-none-any.whl size=113324 sha256=452777027309d18d9c8ee7796c235418edb43510ab3bd637e6517ff61d5f233e\n",
      "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
      "Successfully built pyarabic\n",
      "Installing collected packages: pyarabic, Tashaphyne\n",
      "Successfully installed Tashaphyne-0.3.4.1 pyarabic-0.6.10\n",
      "Collecting Unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 6.1MB/s \n",
      "\u001b[?25hInstalling collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.1.2\n",
      "Collecting aiogoogletrans\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/9c/a1cf0a37077736b3ae8975d482083a720459f6a9bb72d30a5a464ee7957c/aiogoogletrans-3.3.2-py3-none-any.whl\n",
      "Collecting aiohttp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 6.8MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 16.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->aiogoogletrans) (3.7.4.3)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 18.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->aiogoogletrans) (20.3.0)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->aiogoogletrans) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->aiogoogletrans) (2.10)\n",
      "Building wheels for collected packages: idna-ssl\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=f446d6579a2813716ced9b54545579e81167ea4e4a9526baf83bd4243ffb0b76\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
      "Successfully built idna-ssl\n",
      "Installing collected packages: multidict, async-timeout, yarl, idna-ssl, aiohttp, aiogoogletrans\n",
      "Successfully installed aiogoogletrans-3.3.2 aiohttp-3.7.3 async-timeout-3.0.1 idna-ssl-1.1.0 multidict-5.1.0 yarl-1.6.3\n",
      "Collecting cltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/20/42060504debda1fe85808d32ae82344e2efa4343dda15ec0151cf6bfe13d/cltk-0.1.121.tar.gz (625kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 634kB 6.7MB/s \n",
      "\u001b[?25hCollecting gitpython\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/ec98155c501b68dcb11314c7992cd3df6dce193fd763084338a117967d53/GitPython-3.1.12-py3-none-any.whl (159kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 18.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from cltk) (3.2.5)\n",
      "Collecting python-crfsuite\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 747kB 17.3MB/s \n",
      "\u001b[?25hCollecting pyuca\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/aeeee34d88f841aca712a8c18fbd62a33eaad8f2dbe535e87f3c829b02f9/pyuca-1.2-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 29.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from cltk) (3.13)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from cltk) (2019.12.20)\n",
      "Collecting whoosh\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471kB 42.4MB/s \n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 7.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->cltk) (1.15.0)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: cltk\n",
      "  Building wheel for cltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for cltk: filename=cltk-0.1.121-cp36-none-any.whl size=711645 sha256=c3ef04d7555627a51c1db8cba7d51312a6313d12390762d83190df864bdaefa5\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/c9/6b/e60acb6f511ebe008f3e961e894d57598517b25c4cbffbb70f\n",
      "Successfully built cltk\n",
      "Installing collected packages: smmap, gitdb, gitpython, python-crfsuite, pyuca, whoosh, cltk\n",
      "Successfully installed cltk-0.1.121 gitdb-4.0.5 gitpython-3.1.12 python-crfsuite-0.9.7 pyuca-1.2 smmap-3.0.4 whoosh-2.7.4\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip install Tashaphyne\n",
    "!pip install Unidecode\n",
    "!pip install aiogoogletrans\n",
    "!pip install cltk\n",
    "\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from __future__ import unicode_literals\n",
    "from unidecode import unidecode\n",
    "from textblob import TextBlob\n",
    "from aiogoogletrans import Translator\n",
    "import asyncio\n",
    "import unicodedata\n",
    "\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.corpus.arabic.alphabet import *\n",
    "from cltk.stop.arabic.stopword_filter import stopwords_filter as ar_stop_filter\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL8NwzRkFtU5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jAQdrj2FtU8"
   },
   "source": [
    "### Discover Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTqoe30yFtU9"
   },
   "outputs": [],
   "source": [
    "#Stats about Text\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def emoji_counter(sentence):\n",
    "    return emoji.emoji_count(sentence)\n",
    "\n",
    "# train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "# train['char_count'] = train['text'].str.len() ## this also includes spaces\n",
    "# train['avg_char_per_word'] = train['text'].apply(lambda x: avg_word(x))\n",
    "# stop = stopwords.words('arabic')\n",
    "# train['stopwords'] = train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "# train['emoji_count'] = train['text'].apply(lambda x: emoji_counter(x))\n",
    "# train = train.sort_values(by='word_count',ascending=[0])\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q64PQIF6FtU9"
   },
   "source": [
    "### TEXT CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBo5RZpvFtU-"
   },
   "source": [
    "#### Text standarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbQwuJ4jFtU-"
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"arabic\"))\n",
    "stop_word_comp = {\"ØŒ\",\"Ø¢Ø¶\",\"Ø¢Ù…ÙŠÙ†Ù\",\"Ø¢Ù‡\",\"Ø¢Ù‡Ø§Ù‹\",\"Ø¢ÙŠ\",\"Ø£\",\"Ø£Ø¨\",\"Ø£Ø¬Ù„\",\"Ø£Ø¬Ù…Ø¹\",\"Ø£Ø®\",\"Ø£Ø®Ø°\",\"Ø£ØµØ¨Ø­\",\"Ø£Ø¶Ø­Ù‰\",\"Ø£Ù‚Ø¨Ù„\",\"Ø£Ù‚Ù„\",\"Ø£ÙƒØ«Ø±\",\"Ø£Ù„Ø§\",\"Ø£Ù…\",\"Ø£Ù…Ø§\",\"Ø£Ù…Ø§Ù…Ùƒ\",\"Ø£Ù…Ø§Ù…ÙƒÙ\",\"Ø£Ù…Ø³Ù‰\",\"Ø£Ù…Ù‘Ø§\",\"Ø£Ù†\",\"Ø£Ù†Ø§\",\"Ø£Ù†Øª\",\"Ø£Ù†ØªÙ…\",\"Ø£Ù†ØªÙ…Ø§\",\"Ø£Ù†ØªÙ†\",\"Ø£Ù†ØªÙ\",\"Ø£Ù†Ø´Ø£\",\"Ø£Ù†Ù‘Ù‰\",\"Ø£Ùˆ\",\"Ø£ÙˆØ´Ùƒ\",\"Ø£ÙˆÙ„Ø¦Ùƒ\",\"Ø£ÙˆÙ„Ø¦ÙƒÙ…\",\"Ø£ÙˆÙ„Ø§Ø¡\",\"Ø£ÙˆÙ„Ø§Ù„Ùƒ\",\"Ø£ÙˆÙ‘Ù‡Ù’\",\"Ø£ÙŠ\",\"Ø£ÙŠØ§\",\"Ø£ÙŠÙ†\",\"Ø£ÙŠÙ†Ù…Ø§\",\"Ø£ÙŠÙ‘\",\"Ø£ÙÙ†Ù‘Ù\",\"Ø£ÙÙÙŠÙ‘Ù\",\"Ø£ÙÙÙ‘Ù\",\"Ø¥Ø°\",\"Ø¥Ø°Ø§\",\"Ø¥Ø°Ø§Ù‹\",\"Ø¥Ø°Ù…Ø§\",\"Ø¥Ø°Ù†\",\"Ø¥Ù„Ù‰\",\"Ø¥Ù„ÙŠÙƒÙ…\",\"Ø¥Ù„ÙŠÙƒÙ…Ø§\",\"Ø¥Ù„ÙŠÙƒÙ†Ù‘\",\"Ø¥Ù„ÙŠÙƒÙ\",\"Ø¥Ù„ÙÙŠÙ’ÙƒÙ\",\"Ø¥Ù„Ù‘Ø§\",\"Ø¥Ù…Ù‘Ø§\",\"Ø¥Ù†\",\"Ø¥Ù†Ù‘Ù…Ø§\",\"Ø¥ÙŠ\",\"Ø¥ÙŠØ§Ùƒ\",\"Ø¥ÙŠØ§ÙƒÙ…\",\"Ø¥ÙŠØ§ÙƒÙ…Ø§\",\"Ø¥ÙŠØ§ÙƒÙ†\",\"Ø¥ÙŠØ§Ù†Ø§\",\"Ø¥ÙŠØ§Ù‡\",\"Ø¥ÙŠØ§Ù‡Ø§\",\"Ø¥ÙŠØ§Ù‡Ù…\",\"Ø¥ÙŠØ§Ù‡Ù…Ø§\",\"Ø¥ÙŠØ§Ù‡Ù†\",\"Ø¥ÙŠØ§ÙŠ\",\"Ø¥ÙŠÙ‡Ù\",\"Ø¥ÙÙ†Ù‘Ù\",\"Ø§\",\"Ø§Ø¨ØªØ¯Ø£\",\"Ø§Ø«Ø±\",\"Ø§Ø¬Ù„\",\"Ø§Ø­Ø¯\",\"Ø§Ø®Ø±Ù‰\",\"Ø§Ø®Ù„ÙˆÙ„Ù‚\",\"Ø§Ø°Ø§\",\"Ø§Ø±Ø¨Ø¹Ø©\",\"Ø§Ø±ØªØ¯Ù‘\",\"Ø§Ø³ØªØ­Ø§Ù„\",\"Ø§Ø·Ø§Ø±\",\"Ø§Ø¹Ø§Ø¯Ø©\",\"Ø§Ø¹Ù„Ù†Øª\",\"Ø§Ù\",\"Ø§ÙƒØ«Ø±\",\"Ø§ÙƒØ¯\",\"Ø§Ù„Ø£Ù„Ø§Ø¡\",\"Ø§Ù„Ø£Ù„Ù‰\",\"Ø§Ù„Ø§\",\"Ø§Ù„Ø§Ø®ÙŠØ±Ø©\",\"Ø§Ù„Ø§Ù†\",\"Ø§Ù„Ø§ÙˆÙ„\",\"Ø§Ù„Ø§ÙˆÙ„Ù‰\",\"Ø§Ù„ØªÙ‰\",\"Ø§Ù„ØªÙŠ\",\"Ø§Ù„Ø«Ø§Ù†ÙŠ\",\"Ø§Ù„Ø«Ø§Ù†ÙŠØ©\",\"Ø§Ù„Ø°Ø§ØªÙŠ\",\"Ø§Ù„Ø°Ù‰\",\"Ø§Ù„Ø°ÙŠ\",\"Ø§Ù„Ø°ÙŠÙ†\",\"Ø§Ù„Ø³Ø§Ø¨Ù‚\",\"Ø§Ù„Ù\",\"Ø§Ù„Ù„Ø§Ø¦ÙŠ\",\"Ø§Ù„Ù„Ø§ØªÙŠ\",\"Ø§Ù„Ù„ØªØ§Ù†\",\"Ø§Ù„Ù„ØªÙŠØ§\",\"Ø§Ù„Ù„ØªÙŠÙ†\",\"Ø§Ù„Ù„Ø°Ø§Ù†\",\"Ø§Ù„Ù„Ø°ÙŠÙ†\",\"Ø§Ù„Ù„ÙˆØ§ØªÙŠ\",\"Ø§Ù„Ù…Ø§Ø¶ÙŠ\",\"Ø§Ù„Ù…Ù‚Ø¨Ù„\",\"Ø§Ù„ÙˆÙ‚Øª\",\"Ø§Ù„Ù‰\",\"Ø§Ù„ÙŠÙˆÙ…\",\"Ø§Ù…Ø§\",\"Ø§Ù…Ø§Ù…\",\"Ø§Ù…Ø³\",\"Ø§Ù†\",\"Ø§Ù†Ø¨Ø±Ù‰\",\"Ø§Ù†Ù‚Ù„Ø¨\",\"Ø§Ù†Ù‡\",\"Ø§Ù†Ù‡Ø§\",\"Ø§Ùˆ\",\"Ø§ÙˆÙ„\",\"Ø§ÙŠ\",\"Ø§ÙŠØ§Ø±\",\"Ø§ÙŠØ§Ù…\",\"Ø§ÙŠØ¶Ø§\",\"Ø¨\",\"Ø¨Ø§Øª\",\"Ø¨Ø§Ø³Ù…\",\"Ø¨Ø§Ù†\",\"Ø¨Ø®Ù\",\"Ø¨Ø±Ø³\",\"Ø¨Ø³Ø¨Ø¨\",\"Ø¨Ø³Ù‘\",\"Ø¨Ø´ÙƒÙ„\",\"Ø¨Ø¶Ø¹\",\"Ø¨Ø·Ø¢Ù†\",\"Ø¨Ø¹Ø¯\",\"Ø¨Ø¹Ø¶\",\"Ø¨Ùƒ\",\"Ø¨ÙƒÙ…\",\"Ø¨ÙƒÙ…Ø§\",\"Ø¨ÙƒÙ†\",\"Ø¨Ù„\",\"Ø¨Ù„Ù‰\",\"Ø¨Ù…Ø§\",\"Ø¨Ù…Ø§Ø°Ø§\",\"Ø¨Ù…Ù†\",\"Ø¨Ù†\",\"Ø¨Ù†Ø§\",\"Ø¨Ù‡\",\"Ø¨Ù‡Ø§\",\"Ø¨ÙŠ\",\"Ø¨ÙŠØ¯\",\"Ø¨ÙŠÙ†\",\"Ø¨ÙØ³Ù’\",\"Ø¨ÙÙ„Ù’Ù‡Ù\",\"Ø¨ÙØ¦Ù’Ø³Ù\",\"ØªØ§Ù†Ù\",\"ØªØ§Ù†ÙÙƒ\",\"ØªØ¨Ø¯Ù‘Ù„\",\"ØªØ¬Ø§Ù‡\",\"ØªØ­ÙˆÙ‘Ù„\",\"ØªÙ„Ù‚Ø§Ø¡\",\"ØªÙ„Ùƒ\",\"ØªÙ„ÙƒÙ…\",\"ØªÙ„ÙƒÙ…Ø§\",\"ØªÙ…\",\"ØªÙŠÙ†Ùƒ\",\"ØªÙÙŠÙ’Ù†Ù\",\"ØªÙÙ‡\",\"ØªÙÙŠ\",\"Ø«Ù„Ø§Ø«Ø©\",\"Ø«Ù…\",\"Ø«Ù…Ù‘\",\"Ø«Ù…Ù‘Ø©\",\"Ø«ÙÙ…Ù‘Ù\",\"Ø¬Ø¹Ù„\",\"Ø¬Ù„Ù„\",\"Ø¬Ù…ÙŠØ¹\",\"Ø¬ÙŠØ±\",\"Ø­Ø§Ø±\",\"Ø­Ø§Ø´Ø§\",\"Ø­Ø§Ù„ÙŠØ§\",\"Ø­Ø§ÙŠ\",\"Ø­ØªÙ‰\",\"Ø­Ø±Ù‰\",\"Ø­Ø³Ø¨\",\"Ø­Ù…\",\"Ø­ÙˆØ§Ù„Ù‰\",\"Ø­ÙˆÙ„\",\"Ø­ÙŠØ«\",\"Ø­ÙŠØ«Ù…Ø§\",\"Ø­ÙŠÙ†\",\"Ø­ÙŠÙ‘Ù\",\"Ø­ÙØ¨Ù‘ÙØ°ÙØ§\",\"Ø­ÙØªÙ‘ÙÙ‰\",\"Ø­ÙØ°Ø§Ø±Ù\",\"Ø®Ù„Ø§\",\"Ø®Ù„Ø§Ù„\",\"Ø¯ÙˆÙ†\",\"Ø¯ÙˆÙ†Ùƒ\",\"Ø°Ø§\",\"Ø°Ø§Øª\",\"Ø°Ø§Ùƒ\",\"Ø°Ø§Ù†Ùƒ\",\"Ø°Ø§Ù†Ù\",\"Ø°Ù„Ùƒ\",\"Ø°Ù„ÙƒÙ…\",\"Ø°Ù„ÙƒÙ…Ø§\",\"Ø°Ù„ÙƒÙ†\",\"Ø°Ùˆ\",\"Ø°ÙˆØ§\",\"Ø°ÙˆØ§ØªØ§\",\"Ø°ÙˆØ§ØªÙŠ\",\"Ø°ÙŠØª\",\"Ø°ÙŠÙ†Ùƒ\",\"Ø°ÙÙŠÙ’Ù†Ù\",\"Ø°ÙÙ‡\",\"Ø°ÙÙŠ\",\"Ø±Ø§Ø­\",\"Ø±Ø¬Ø¹\",\"Ø±ÙˆÙŠØ¯Ùƒ\",\"Ø±ÙŠØ«\",\"Ø±ÙØ¨Ù‘Ù\",\"Ø²ÙŠØ§Ø±Ø©\",\"Ø³Ø¨Ø­Ø§Ù†\",\"Ø³Ø±Ø¹Ø§Ù†\",\"Ø³Ù†Ø©\",\"Ø³Ù†ÙˆØ§Øª\",\"Ø³ÙˆÙ\",\"Ø³ÙˆÙ‰\",\"Ø³ÙØ§Ø¡Ù\",\"Ø³ÙØ§Ø¡ÙÙ…ÙØ§\",\"Ø´Ø¨Ù‡\",\"Ø´Ø®ØµØ§\",\"Ø´Ø±Ø¹\",\"Ø´ÙØªÙ‘ÙØ§Ù†Ù\",\"ØµØ§Ø±\",\"ØµØ¨Ø§Ø­\",\"ØµÙØ±\",\"ØµÙ‡Ù\",\"ØµÙ‡Ù’\",\"Ø¶Ø¯\",\"Ø¶Ù…Ù†\",\"Ø·Ø§Ù‚\",\"Ø·Ø§Ù„Ù…Ø§\",\"Ø·ÙÙ‚\",\"Ø·ÙÙ‚\",\"Ø¸Ù„Ù‘\",\"Ø¹Ø§Ø¯\",\"Ø¹Ø§Ù…\",\"Ø¹Ø§Ù…Ø§\",\"Ø¹Ø§Ù…Ø©\",\"Ø¹Ø¯Ø§\",\"Ø¹Ø¯Ø©\",\"Ø¹Ø¯Ø¯\",\"Ø¹Ø¯Ù…\",\"Ø¹Ø³Ù‰\",\"Ø¹Ø´Ø±\",\"Ø¹Ø´Ø±Ø©\",\"Ø¹Ù„Ù‚\",\"Ø¹Ù„Ù‰\",\"Ø¹Ù„ÙŠÙƒ\",\"Ø¹Ù„ÙŠÙ‡\",\"Ø¹Ù„ÙŠÙ‡Ø§\",\"Ø¹Ù„Ù‘Ù‹\",\"Ø¹Ù†\",\"Ø¹Ù†Ø¯\",\"Ø¹Ù†Ø¯Ù…Ø§\",\"Ø¹ÙˆØ¶\",\"Ø¹ÙŠÙ†\",\"Ø¹ÙØ¯ÙØ³Ù’\",\"Ø¹ÙÙ…Ù‘ÙØ§\",\"ØºØ¯Ø§\",\"ØºÙŠØ±\",\"Ù€\",\"Ù\",\"ÙØ§Ù†\",\"ÙÙ„Ø§Ù†\",\"ÙÙˆ\",\"ÙÙ‰\",\"ÙÙŠ\",\"ÙÙŠÙ…\",\"ÙÙŠÙ…Ø§\",\"ÙÙŠÙ‡\",\"ÙÙŠÙ‡Ø§\",\"Ù‚Ø§Ù„\",\"Ù‚Ø§Ù…\",\"Ù‚Ø¨Ù„\",\"Ù‚Ø¯\",\"Ù‚Ø·Ù‘\",\"Ù‚Ù„Ù…Ø§\",\"Ù‚ÙˆØ©\",\"ÙƒØ£Ù†Ù‘Ù…Ø§\",\"ÙƒØ£ÙŠÙ†\",\"ÙƒØ£ÙŠÙ‘\",\"ÙƒØ£ÙŠÙ‘Ù†\",\"ÙƒØ§Ø¯\",\"ÙƒØ§Ù†\",\"ÙƒØ§Ù†Øª\",\"ÙƒØ°Ø§\",\"ÙƒØ°Ù„Ùƒ\",\"ÙƒØ±Ø¨\",\"ÙƒÙ„\",\"ÙƒÙ„Ø§\",\"ÙƒÙ„Ø§Ù‡Ù…Ø§\",\"ÙƒÙ„ØªØ§\",\"ÙƒÙ„Ù…\",\"ÙƒÙ„ÙŠÙƒÙ…Ø§\",\"ÙƒÙ„ÙŠÙ‡Ù…Ø§\",\"ÙƒÙ„Ù‘Ù…Ø§\",\"ÙƒÙ„Ù‘ÙØ§\",\"ÙƒÙ…\",\"ÙƒÙ…Ø§\",\"ÙƒÙŠ\",\"ÙƒÙŠØª\",\"ÙƒÙŠÙ\",\"ÙƒÙŠÙÙ…Ø§\",\"ÙƒÙØ£ÙÙ†Ù‘Ù\",\"ÙƒÙØ®\",\"Ù„Ø¦Ù†\",\"Ù„Ø§\",\"Ù„Ø§Øª\",\"Ù„Ø§Ø³ÙŠÙ…Ø§\",\"Ù„Ø¯Ù†\",\"Ù„Ø¯Ù‰\",\"Ù„Ø¹Ù…Ø±\",\"Ù„Ù‚Ø§Ø¡\",\"Ù„Ùƒ\",\"Ù„ÙƒÙ…\",\"Ù„ÙƒÙ…Ø§\",\"Ù„ÙƒÙ†\",\"Ù„ÙƒÙ†Ù‘ÙÙ…Ø§\",\"Ù„ÙƒÙŠ\",\"Ù„ÙƒÙŠÙ„Ø§\",\"Ù„Ù„Ø§Ù…Ù…\",\"Ù„Ù…\",\"Ù„Ù…Ø§\",\"Ù„Ù…Ù‘Ø§\",\"Ù„Ù†\",\"Ù„Ù†Ø§\",\"Ù„Ù‡\",\"Ù„Ù‡Ø§\",\"Ù„Ùˆ\",\"Ù„ÙˆÙƒØ§Ù„Ø©\",\"Ù„ÙˆÙ„Ø§\",\"Ù„ÙˆÙ…Ø§\",\"Ù„ÙŠ\",\"Ù„ÙØ³Ù’ØªÙ\",\"Ù„ÙØ³Ù’ØªÙ\",\"Ù„ÙØ³Ù’ØªÙÙ…\",\"Ù„ÙØ³Ù’ØªÙÙ…ÙØ§\",\"Ù„ÙØ³Ù’ØªÙÙ†Ù‘Ù\",\"Ù„ÙØ³Ù’ØªÙ\",\"Ù„ÙØ³Ù’Ù†Ù\",\"Ù„ÙØ¹ÙÙ„Ù‘Ù\",\"Ù„ÙÙƒÙÙ†Ù‘Ù\",\"Ù„ÙÙŠÙ’ØªÙ\",\"Ù„ÙÙŠÙ’Ø³Ù\",\"Ù„ÙÙŠÙ’Ø³ÙØ§\",\"Ù„ÙÙŠÙ’Ø³ÙØªÙØ§\",\"Ù„ÙÙŠÙ’Ø³ÙØªÙ’\",\"Ù„ÙÙŠÙ’Ø³ÙÙˆØ§\",\"Ù„ÙÙØ³Ù’Ù†ÙØ§\",\"Ù…Ø§\",\"Ù…Ø§Ø§Ù†ÙÙƒ\",\"Ù…Ø§Ø¨Ø±Ø­\",\"Ù…Ø§Ø¯Ø§Ù…\",\"Ù…Ø§Ø°Ø§\",\"Ù…Ø§Ø²Ø§Ù„\",\"Ù…Ø§ÙØªØ¦\",\"Ù…Ø§ÙŠÙˆ\",\"Ù…ØªÙ‰\",\"Ù…Ø«Ù„\",\"Ù…Ø°\",\"Ù…Ø³Ø§Ø¡\",\"Ù…Ø¹\",\"Ù…Ø¹Ø§Ø°\",\"Ù…Ù‚Ø§Ø¨Ù„\",\"Ù…ÙƒØ§Ù†ÙƒÙ…\",\"Ù…ÙƒØ§Ù†ÙƒÙ…Ø§\",\"Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘\",\"Ù…ÙƒØ§Ù†ÙÙƒ\",\"Ù…Ù„ÙŠØ§Ø±\",\"Ù…Ù„ÙŠÙˆÙ†\",\"Ù…Ù…Ø§\",\"Ù…Ù…Ù†\",\"Ù…Ù†\",\"Ù…Ù†Ø°\",\"Ù…Ù†Ù‡Ø§\",\"Ù…Ù‡\",\"Ù…Ù‡Ù…Ø§\",\"Ù…ÙÙ†Ù’\",\"Ù…ÙÙ†\",\"Ù†Ø­Ù†\",\"Ù†Ø­Ùˆ\",\"Ù†Ø¹Ù…\",\"Ù†ÙØ³\",\"Ù†ÙØ³Ù‡\",\"Ù†Ù‡Ø§ÙŠØ©\",\"Ù†ÙØ®Ù’\",\"Ù†ÙØ¹ÙÙ…Ù‘Ø§\",\"Ù†ÙØ¹Ù’Ù…Ù\",\"Ù‡Ø§\",\"Ù‡Ø§Ø¤Ù…\",\"Ù‡Ø§ÙƒÙ\",\"Ù‡Ø§Ù‡Ù†Ø§\",\"Ù‡Ø¨Ù‘\",\"Ù‡Ø°Ø§\",\"Ù‡Ø°Ù‡\",\"Ù‡ÙƒØ°Ø§\",\"Ù‡Ù„\",\"Ù‡Ù„Ù…Ù‘Ù\",\"Ù‡Ù„Ù‘Ø§\",\"Ù‡Ù…\",\"Ù‡Ù…Ø§\",\"Ù‡Ù†\",\"Ù‡Ù†Ø§\",\"Ù‡Ù†Ø§Ùƒ\",\"Ù‡Ù†Ø§Ù„Ùƒ\",\"Ù‡Ùˆ\",\"Ù‡ÙŠ\",\"Ù‡ÙŠØ§\",\"Ù‡ÙŠØª\",\"Ù‡ÙŠÙ‘Ø§\",\"Ù‡ÙØ¤Ù„Ø§Ø¡\",\"Ù‡ÙØ§ØªØ§Ù†Ù\",\"Ù‡ÙØ§ØªÙÙŠÙ’Ù†Ù\",\"Ù‡ÙØ§ØªÙÙ‡\",\"Ù‡ÙØ§ØªÙÙŠ\",\"Ù‡ÙØ¬Ù’\",\"Ù‡ÙØ°Ø§\",\"Ù‡ÙØ°Ø§Ù†Ù\",\"Ù‡ÙØ°ÙÙŠÙ’Ù†Ù\",\"Ù‡ÙØ°ÙÙ‡\",\"Ù‡ÙØ°ÙÙŠ\",\"Ù‡ÙÙŠÙ’Ù‡ÙØ§ØªÙ\",\"Ùˆ\",\"Ùˆ6\",\"ÙˆØ§\",\"ÙˆØ§Ø­Ø¯\",\"ÙˆØ§Ø¶Ø§Ù\",\"ÙˆØ§Ø¶Ø§ÙØª\",\"ÙˆØ§ÙƒØ¯\",\"ÙˆØ§Ù†\",\"ÙˆØ§Ù‡Ø§Ù‹\",\"ÙˆØ§ÙˆØ¶Ø­\",\"ÙˆØ±Ø§Ø¡ÙÙƒ\",\"ÙˆÙÙŠ\",\"ÙˆÙ‚Ø§Ù„\",\"ÙˆÙ‚Ø§Ù„Øª\",\"ÙˆÙ‚Ø¯\",\"ÙˆÙ‚Ù\",\"ÙˆÙƒØ§Ù†\",\"ÙˆÙƒØ§Ù†Øª\",\"ÙˆÙ„Ø§\",\"ÙˆÙ„Ù…\",\"ÙˆÙ…Ù†\",\"Ù…ÙÙ†\",\"ÙˆÙ‡Ùˆ\",\"ÙˆÙ‡ÙŠ\",\"ÙˆÙŠÙƒØ£Ù†Ù‘\",\"ÙˆÙÙŠÙ’\",\"ÙˆÙØ´Ù’ÙƒÙØ§Ù†ÙÙ\",\"ÙŠÙƒÙˆÙ†\",\"ÙŠÙ…ÙƒÙ†\",\"ÙŠÙˆÙ…\",\"Ù‘Ø£ÙŠÙ‘Ø§Ù†\"}\n",
    "ArListem = ArabicLightStemmer()\n",
    "\n",
    "\n",
    "def stem(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    cleaned = list()\n",
    "    for w in words:\n",
    "        ArListem.light_stem(w)\n",
    "        cleaned.append(ArListem.get_root())\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    noise = re.compile(\"\"\" Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return araby.strip_tashkeel(text)\n",
    "    \n",
    "def remove_stop_words(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnOMen7AFtVA"
   },
   "source": [
    "#### Texts from Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn4_fkueFtVB"
   },
   "source": [
    "#### Deal with Hashtags in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoX_mHckFtVB"
   },
   "outputs": [],
   "source": [
    "def split_hashtag_to_words(tag):\n",
    "    tag = tag.replace('#','')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1 :\n",
    "        \n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def clean_hashtag(text):\n",
    "    words = text.split()\n",
    "    text = list()\n",
    "    for word in words:\n",
    "        if is_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    return \" \".join(text)\n",
    "def is_hashtag(word):\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def extract_hashtag(text):\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "    for word in hash_list :\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D4_ZhUlFtVC"
   },
   "source": [
    "#### Dealing with emojis in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0Mui9y2FtVC"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/Emojis/emojis.csv','r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    emojis_ar = {}\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n').split(';')\n",
    "        emojis_ar.update({line[0].strip():line[1].strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdkqSZ2GFtVC"
   },
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def emoji_native_translation(text):\n",
    "    text = text.lower()\n",
    "    loves = [\"<3\", \"â™¥\",'â¤']\n",
    "    smilefaces = []\n",
    "    sadfaces = []\n",
    "    neutralfaces = []\n",
    "\n",
    "    eyes = [\"8\",\":\",\"=\",\";\"]\n",
    "    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n",
    "    for e in eyes:\n",
    "        for n in nose:\n",
    "            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n",
    "                smilefaces.append(e+n+s)\n",
    "                smilefaces.append(e+s)\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                sadfaces.append(e+n+s)\n",
    "                sadfaces.append(e+s)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(e+n+s)\n",
    "                neutralfaces.append(e+s)\n",
    "            #reversed\n",
    "            for s in [\"\\(\", \"\\[\", \"{\"]:\n",
    "                smilefaces.append(s+n+e)\n",
    "                smilefaces.append(s+e)\n",
    "            for s in [\"\\)\", \"\\]\", \"}\"]:\n",
    "                sadfaces.append(s+n+e)\n",
    "                sadfaces.append(s+e)\n",
    "            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n",
    "                neutralfaces.append(s+n+e)\n",
    "                neutralfaces.append(s+e)\n",
    "\n",
    "    smilefaces = list(set(smilefaces))\n",
    "    sadfaces = list(set(sadfaces))\n",
    "    neutralfaces = list(set(neutralfaces))\n",
    "    t = []\n",
    "    for w in text.split():\n",
    "        if w in loves:\n",
    "            t.append(\"Ø­Ø¨\")\n",
    "        elif w in smilefaces:\n",
    "            t.append(\"Ù…Ø¶Ø­Ùƒ\")\n",
    "        elif w in neutralfaces:\n",
    "            t.append(\"Ø¹Ø§Ø¯ÙŠ\")\n",
    "        elif w in sadfaces:\n",
    "            t.append(\"Ù…Ø­Ø²Ù†\")\n",
    "        else:\n",
    "            t.append(w)\n",
    "    newText = \" \".join(t)\n",
    "    return newText\n",
    "\n",
    "def is_emoji(word):\n",
    "    if word in emojis_ar:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "translator = Translator()\n",
    "loop = asyncio.get_event_loop()\n",
    "def translate_emojis(words):\n",
    "    word_list = list()\n",
    "    words_to_translate = list()\n",
    "    for word in words :\n",
    "        t = emojis_ar.get(word.get('emoji'),None)\n",
    "        if t is None:\n",
    "            word.update({'translation':'Ø¹Ø§Ø¯ÙŠ','translated':True})\n",
    "            #words_to_translate.append('normal')\n",
    "        else:\n",
    "            word.update({'translated':False,'translation':t})\n",
    "            words_to_translate.append(t.replace(':','').replace('_',' '))\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def emoji_unicode_translation(text):\n",
    "    text = add_space(text)\n",
    "    words = text.split()\n",
    "    text_list = list()\n",
    "    emojis_list = list()\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        if is_emoji(word):\n",
    "            emojis_list.append({'emoji':word,'emplacement':c})\n",
    "        else:\n",
    "            text_list.append(word)\n",
    "        c+=1\n",
    "    emojis_translated = translate_emojis(emojis_list)\n",
    "    for em in emojis_translated:\n",
    "        text_list.insert(em.get('emplacement'),em.get('translation'))\n",
    "    text = \" \".join(text_list)\n",
    "    return text\n",
    "    \n",
    "def clean_emoji(text):\n",
    "    # text = emoji_native_translation(text)\n",
    "    # text = emoji_unicode_translation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMg_JA7AFtVE"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "\n",
    "    if not isinstance(text, str) : \n",
    "      # print(text)\n",
    "      text=str(text)\n",
    "    text = re.sub('#\\d+K\\d+', ' ', text)  # years like 2K19\n",
    "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
    "    text = re.sub('@[^\\s]+',' ',text)\n",
    "    text = clean_hashtag(text)\n",
    "    text = clean_emoji(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZbwpKjZFtVF"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ## Clean for tweets\n",
    "    text = clean_tweet(text)\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    ## Remove Emojis\n",
    "    text = remove_emoji(text)\n",
    "    ## Convert text to lowercases\n",
    "    text = text.lower()\n",
    "    ## Remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    ## Remove numbers\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    ## Remove Tashkeel\n",
    "    text = normalizeArabic(text)\n",
    "    #text = re.sub('\\W+', ' ', text)\n",
    "    text = re.sub('[A-Za-z]+',' ',text)\n",
    "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    #Stemming\n",
    "    #text = stem(text)\n",
    "    return text\n",
    "\n",
    "# train['text'] = train['text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCDNEZtBFtVG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf9e11K6FtVH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJzhmX4nFtVH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "p9CoMsPNFtVI",
    "outputId": "ef298824-2c45-4ca6-c051-e35243d54db0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Ø§Ù†Ø§ Ù…ØªØ§ÙƒØ¯ ÙƒØ¯Ù‡'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"@galalroushdy @KajoTm Ù…Ø§ Ø§Ù†Ø§ Ù…ØªØ£ÙƒØ¯ Ù…Ù† ÙƒØ¯Ù‡ ğŸ˜‚ğŸ˜‚ğŸ˜‚\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Jxh4INJmONAA",
    "outputId": "2d3e0c2d-c371-4de6-c736-31aa422924b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Ø§Ù†Ø§ Ø®Ø§ÙŠÙ Ù„Ù‚ÙŠ Ø«Ø§Ù†ÙŠ'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('RT @alhanoufj__: Ø§Ù†Ø§ Ø®Ø§ÙŠÙ Ù„Ù‚Ù‰ Ø«Ø§Ù†ÙŠğŸ§ğŸ’• https://t...\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rfBWWB6S-4Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2i8O2E-PPp0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/tweetsDataset/tweets.csv',sep=',',names=['ID','Tweet','Class']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "_QmiGXWSTBIG",
    "outputId": "a7b10bd1-5462-4587-d718-02453dd5d46d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94827</th>\n",
       "      <td>389849012956057600</td>\n",
       "      <td>ÙˆÙ†Ùˆ ÙˆÙƒØ¯ ØªÙ…Ù… ØªØ­Ø¯ Ø²ÙˆØ¯ Ø¬Ù†Ù† Ø¶Ø¨Ø· Ø¨Ù„Ø³ Ø¯ Ø­ÙˆÙ„ Ù‡ØªÙ Ø¬Ù…Ù‡Ø±...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94828</th>\n",
       "      <td>37337</td>\n",
       "      <td></td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94829</th>\n",
       "      <td>884428772866498560</td>\n",
       "      <td>ÙˆÙ†Ùˆ ÙˆÙƒØ¯ Ø³Ù„Ù Ù‚Ø·Ø± Ùª Ø¬ÙŠØ´ ØµØ±ØµØ±</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94830</th>\n",
       "      <td>716683864257781760</td>\n",
       "      <td>Ø²ÙŠÙ† Ù„ Ø¯Ù‚Ù‚ Ù†Ø®Ø¨ Ø¯Ø®Ù„ Ù‡Ø¯Ù Ø´ØºÙ„ Ø«ÙˆÙ† ÙˆÙ†Ùˆ ÙˆØ«Ù‚ ÙˆÙ†Ùƒ Ø´ÙØª ...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94831</th>\n",
       "      <td>449178499962908673</td>\n",
       "      <td>Ø¯ÙˆÙ† Ø¯Ø­Ø­ Ù„ÙˆÙ… ÙˆÙ†Ùˆ ÙˆØ«Ù‚ ÙÙƒØ±</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID  ...  Class\n",
       "94827  389849012956057600  ...  trust\n",
       "94828               37337  ...  trust\n",
       "94829  884428772866498560  ...  trust\n",
       "94830  716683864257781760  ...  trust\n",
       "94831  449178499962908673  ...  trust\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = CorpusImporter('arabic')\n",
    "c.list_corpora\n",
    "c.import_corpus('arabic_text_perseus')  # ~/cltk_data/arabic/text/arabic_text_perseus/\n",
    "word_tokenizer = WordTokenizer('arabic')\n",
    "import nltk.stem.arlstem\n",
    "df['Tweet'] =df['Tweet'].apply(lambda x: clean_text(x))\n",
    "df['Tweet'] =df['Tweet'].apply(lambda x: ' '.join(word_tokenizer.tokenize(x)))\n",
    "df['Tweet'] =df['Tweet'].apply(lambda x: ' '.join(ar_stop_filter(x)))\n",
    "df['Tweet'] =df['Tweet'].apply(lambda x: araby.strip_tashkeel(x))\n",
    "df['Tweet'] =df['Tweet'].apply(lambda x: stem(x))\n",
    "\n",
    "#  df = pd.DataFrame(np.random.randn(100, 2))\n",
    "\n",
    "# text = 'Ø§Ù„Ù„Ù‘ÙØºÙØ©Ù Ø§Ù„Ù’Ø¹ÙØ±ÙØ¨ÙÙŠÙ‘ÙØ©Ù Ø¬ÙÙ…ÙÙŠÙ„ÙØ©ÙŒ.'\n",
    "# word_tokenizer.tokenize(x)\n",
    "# araby.strip_tashkeel(x)\n",
    "\n",
    "df[].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBA3W1umb6H9"
   },
   "outputs": [],
   "source": [
    "df_not_empty =df[df[\"Tweet\"] != '']\n",
    "df_not_empty.to_csv('/content/drive/MyDrive/beforSplitedDataset/tweets.csv', index=False)\n",
    "\n",
    "# msk = np.random.rand(len(df)) < 0.8\n",
    "# train = df[msk]\n",
    "# test = df[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "IRkZeHossVSE",
    "outputId": "6cec1b6d-7cb2-402e-a114-705a1426a68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16004\n",
      "64843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295820374774513667</td>\n",
       "      <td>Ø±Ø¯Ø¬Ø±Ø² ÙˆÙ†Ùˆ Ø³ÙˆØ¡ Ø¹Ø¨Ù† Ø´Ø¨Ø¨ Ù…Ù†Ø­ Ø®Ø¨Ø± Ø´ÙˆØ± Ø¹Ø±Ù Ù‚Ù…Ù… Ù„Ø¹Ø¨ Ø±Ø¨Ù„</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353619287652581376</td>\n",
       "      <td>Ø´ÙÙ ÙˆÙ†Ùˆ Ø¹ÙˆÙ† ÙˆÙ†Ùˆ Ø¬Ù†Ù†</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>888346524714979329</td>\n",
       "      <td>ÙˆÙÙ‚ ÙˆÙ†Ùˆ Ø¹ØµØ¨ Ù‚Ø³Ø³ Ù†Ù‚Ø¶ Ø²Ø¬Ø¬ ÙƒÙˆÙ… Ø¨Ø±Ø± Ø­ÙŠÙ„ ÙƒØ°Ø¨ Ø´Ø¹Ø± Ø­Ø¨...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>870974517983752193</td>\n",
       "      <td>ÙƒØ±Ø± ÙˆÙ†Ùˆ Ø³ÙˆØ¡ Ø¬Ø¯Ø¯ Ø¬Ø±Ø¨ Ø­Ù„Ù„ Ø®Ø¨Ø± ÙŠØªÙ… Ù„ØºØº Ø®ÙØ¶ Ù„Ø³Ø¹</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>886636321342619653</td>\n",
       "      <td>ÙˆÙ†Ùˆ Ø¹ØµØ¨ Ø¨Ø³Ø· Ø­Ù„Ùˆ Ù‡Ù†Ø³ Ø´ÙƒÙ„</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                              Tweet  Class\n",
       "0  295820374774513667  Ø±Ø¯Ø¬Ø±Ø² ÙˆÙ†Ùˆ Ø³ÙˆØ¡ Ø¹Ø¨Ù† Ø´Ø¨Ø¨ Ù…Ù†Ø­ Ø®Ø¨Ø± Ø´ÙˆØ± Ø¹Ø±Ù Ù‚Ù…Ù… Ù„Ø¹Ø¨ Ø±Ø¨Ù„  anger\n",
       "1  353619287652581376                                Ø´ÙÙ ÙˆÙ†Ùˆ Ø¹ÙˆÙ† ÙˆÙ†Ùˆ Ø¬Ù†Ù†  anger\n",
       "2  888346524714979329  ÙˆÙÙ‚ ÙˆÙ†Ùˆ Ø¹ØµØ¨ Ù‚Ø³Ø³ Ù†Ù‚Ø¶ Ø²Ø¬Ø¬ ÙƒÙˆÙ… Ø¨Ø±Ø± Ø­ÙŠÙ„ ÙƒØ°Ø¨ Ø´Ø¹Ø± Ø­Ø¨...  anger\n",
       "3  870974517983752193        ÙƒØ±Ø± ÙˆÙ†Ùˆ Ø³ÙˆØ¡ Ø¬Ø¯Ø¯ Ø¬Ø±Ø¨ Ø­Ù„Ù„ Ø®Ø¨Ø± ÙŠØªÙ… Ù„ØºØº Ø®ÙØ¶ Ù„Ø³Ø¹  anger\n",
       "6  886636321342619653                            ÙˆÙ†Ùˆ Ø¹ØµØ¨ Ø¨Ø³Ø· Ø­Ù„Ùˆ Ù‡Ù†Ø³ Ø´ÙƒÙ„  anger"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test))\n",
    "print(len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H85SwNF4wVIV"
   },
   "outputs": [],
   "source": [
    "train = df.sample(frac = 0.75) \n",
    "test = df.drop(part_75.index) \n",
    "  \n",
    "train.to_csv('/content/drive/MyDrive/splitedDataset/train.csv', index=False)\n",
    "test.to_csv('/content/drive/MyDrive/splitedDataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NUAyPa4wdl_",
    "outputId": "c5d3a9ba-f352-4a1c-bd8a-21beb3cd9fec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62443    ÙˆÙ†Ùˆ Ø²Ø¹Ù„ Ù‚Ù„ØªÙ„ Ø¶ÙˆÙ‚ ØµØ¯Ù‚ Ù…Ø´Ø´ Ù‡Ø³Ø¨ ØªÙ†Ù… Ø²Ø¹Ù„ Ø·ÙˆÙ„ Ø¹Ù…Ø± Ùƒ...\n",
       "31420                              Ù†ÙˆØ³ ÙˆØ¯Ùƒ Ù‚ÙˆÙ„ Ø¹Ù„Ø´ Ø¨Ø³Ø· Ù„Ø­Ù†\n",
       "40938                                  Ù‡Ø²Ø² Ø¯Ø±Ø¨ Ø­Ù†Ù† ÙˆÙ†Ùˆ ØºÙŠØ¨\n",
       "26102                                  ÙˆÙ†Ùˆ Ø®ÙˆÙ ÙØªØ­ ÙˆÙ„Ø³ Ù„Ù‡Ù‡\n",
       "12283    Ù‚ÙˆÙ„ Ù„Ù„Ù„ Ù†ÙˆÙ… Ø³ÙˆØ¹ Ù†Ø³Ø³ Ø¨ÙŠØª Ù„Ø¬Ø¬ Ø¹Ø´Ø´ Ù‚ÙˆÙ… Ø³Ù„Ù„ Ø®Ù„Ùˆ Ù„Ù‡...\n",
       "                               ...                        \n",
       "59711      Ù…ÙŠØ³ ÙˆÙ†Ùˆ Ø­Ø²Ø² Ù„Ø¹Ø¨ Ø­Ù…Ø¯ Ù†ÙˆØ± ÙˆØµÙ„ Ø®Ø¨Ø± Ù‚ÙˆÙ Ø²Ù…Ù„ Ù‡Ø¯Ø¯ Ù…Ù„Ø¯\n",
       "51911          Ù‡Ø¯Ø¯ Ø±Ø¬Ù„ Ø¨Ù„Ù„ Ù…Ù„Ù„ Ù†Ø²Ù„ Ø³Ù‚Ù Ø¯Ø®Ø® Ø²Ø±Ù‚ ØµØ¹Ø¯ Ù‚ÙˆÙ„ ØºØ±Ù\n",
       "85084                  ÙˆÙ†Ùˆ ÙƒØ¯Ø¯ ÙˆÙ†Ùˆ Ø±Ø¬Ø¬ Ø¯Ø±Ø¨ Ù‡Ù„Ù„ Ø³ÙƒØ± Ø²ÙˆØ¬ Ø´Ø´Ø´\n",
       "94071    Ø¨Ø³Ø· ÙØ±Ù‚ Ù‚Ø±Ø¨ Ø¡Ø³Ø³ Ù„ÙˆÙ† Ø´Ø¨Ø¨ Ù…Ø´Ø´ Ø­Ø³Ø³ Ù„Ù‚Ù‚ Ù…Ø¹Ùˆ Ø¹Ø¯Ùˆ ÙƒØ±...\n",
       "48030    Ø³Ù…Ù… Ù…ÙŠØ± ÙÙˆØ² Ù„Ø­Ø³ Ø­Ø¯Ø¯ Ø¬Ø¯Ø¯ Ø±Ø¡ÙŠ Ø´Ø¨Ø¨ Ø­Ù„Ùˆ Ø­Ù„Ù„ ÙƒÙØ¡ ÙˆØ¬...\n",
       "Name: Tweet, Length: 60635, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRsSXHUHwd33"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjLQHsoHweMD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OV0I2omrweyz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCPjnywHwZJj"
   },
   "source": [
    "**bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-A0LlZBb2UY",
    "outputId": "81bcb824-ce0e-40e5-e59e-125c3fc5c317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80847"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "df.replace(\"\", nan_value, inplace=True)\n",
    "df.dropna(subset = [\"Tweet\"], inplace=True)\n",
    "len(df['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2VXMbfgdo5S"
   },
   "outputs": [],
   "source": [
    "df.to_csv('/content/drive/MyDrive/cleanedDataset/tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owceuq0gsC2c",
    "outputId": "eaaa2000-c362-4445-bf10-86d942c20598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger: 1161\n",
      "Trust: 13416\n",
      "Happiness: 22847\n",
      "Sadness: 12937\n",
      "Fear: 11214\n",
      "Anticipation: 11806\n",
      "Disgust: 775\n",
      "Surprise: 6691\n"
     ]
    }
   ],
   "source": [
    "print('Anger:',len(df[df[\"Class\"] == 'anger']))\n",
    "print('Trust:',len(df[df[\"Class\"] == 'trust']))\n",
    "print('Happiness:',len(df[df[\"Class\"] == 'happiness']))\n",
    "print('Sadness:',len(df[df[\"Class\"] == 'sadness']))\n",
    "print('Fear:',len(df[df[\"Class\"] == 'fear']))\n",
    "print('Anticipation:',len(df[df[\"Class\"] == 'anticipation']))\n",
    "print('Disgust:',len(df[df[\"Class\"] == 'disgust']))\n",
    "print('Surprise:',len(df[df[\"Class\"] == 'surprise']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "ylZsysjzsxpy",
    "outputId": "4bddead4-7a75-4f2d-a33b-3157b2eb29f5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c8139216c0bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arabic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ÙŠØ±Ù‚ØµÙ€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€ÙˆÙ† Ø§Ù„Ù„Ù‘ÙØºÙØ©Ù Ø§Ù„Ù’Ø¹ÙØ±ÙØ¨ÙÙŠÙ‘ÙØ©Ù Ø¬ÙÙ…ÙÙŠÙ„ÙØ©ÙŒ.  '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstrip_tatweel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# ' '.join (word_tokenizer.tokenize(text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# araby.strip_tashkeel(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strip_tatweel' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "c = CorpusImporter('arabic')\n",
    "c.list_corpora\n",
    "c.import_corpus('arabic_text_perseus')  # ~/cltk_data/arabic/text/arabic_text_perseus/\n",
    "word_tokenizer = WordTokenizer('arabic')\n",
    "text = 'ÙŠØ±Ù‚ØµÙ€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€ÙˆÙ† Ø§Ù„Ù„Ù‘ÙØºÙØ©Ù Ø§Ù„Ù’Ø¹ÙØ±ÙØ¨ÙÙŠÙ‘ÙØ©Ù Ø¬ÙÙ…ÙÙŠÙ„ÙØ©ÙŒ.  '\n",
    "# strip_tatweel(text)\n",
    "' '.join (word_tokenizer.tokenize(text))\n",
    "# araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG3POj78uXIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oUfYac0sWTP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqYvXfIbsWmo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcAOkhXAsW4a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWlNr_lGsXFn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDaj8CclsXS-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAcY-GOJsXde"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWZWOjQ6sXmp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuMPuxVKso13"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "DUMMY CODE NOT USED\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_pvfoY6hsXv9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7anxc4lsX5V"
   },
   "outputs": [],
   "source": [
    "# df['Tweet'].apply(lambda x: emoji_counter(x))\n",
    "# df['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "# ### Rare words removal\n",
    "# freq = pd.Series(' '.join(train['text']).split()).value_counts()[-50:]\n",
    "# freq = list(freq.index)\n",
    "# train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "# ### Remove Numbers from text\n",
    "\n",
    "\n",
    "# ### Discouver Data again after cleaning\n",
    "# train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "# train['char_count'] = train['text'].str.len() ## this also includes spaces\n",
    "# train['avg_char_per_word'] = train['text'].apply(lambda x: avg_word(x))\n",
    "# stop = stopwords.words('arabic')\n",
    "# train['stopwords'] = train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "# train['emoji_count'] = train['text'].apply(lambda x: emoji_counter(x))\n",
    "# train = train.sort_values(by='word_count',ascending=[0])\n",
    "# final = []\n",
    "# for index, row in train.iterrows():\n",
    "#     if len(row['text'].split()) > 3:\n",
    "#         final.append([row['text'],row['sentiment']])\n",
    "# df = pd.DataFrame(final)\n",
    "# df.columns = ['text','sentiment']\n",
    "# df.to_csv('final_nice.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Arabic Tweets Preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
